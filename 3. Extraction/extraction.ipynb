{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‰. ä¿¡æ¯æŠ½å–(Extraction)\n",
    "\n",
    "- Extractionæ˜¯ä»ä¸€æ®µæ–‡æœ¬ä¸­è§£ææ•°æ®çš„è¿‡ç¨‹\n",
    "- é€šå¸¸ä¸Extraction parserä¸€èµ·ä½¿ç”¨ï¼Œä»¥æ„å»ºæ•°æ®\n",
    "\n",
    "1. ä»å¥å­ä¸­æå–ç»“æ„åŒ–è¡Œä»¥æ’å…¥æ•°æ®åº“\n",
    "2. ä»é•¿æ–‡æ¡£ä¸­æå–å¤šè¡Œä»¥æ’å…¥æ•°æ®åº“\n",
    "3. ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–å‚æ•°ä»¥è¿›è¡Œ API è°ƒç”¨\n",
    "4. æœ€è¿‘æœ€ç«çš„ Extraction åº“æ˜¯ KOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. æ‰‹åŠ¨æ ¼å¼è½¬æ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from models import azure_chat_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "\n",
    "# è§£æè¾“å‡ºå¹¶è·å–ç»“æ„åŒ–çš„æ•°æ®\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Extraction\n",
    "instructions = \"\"\"\n",
    "You will be given a sentence with fruit names, extract those fruit names and assign an emoji to them\n",
    "Return the fruit name and emojis in a python dictionary\n",
    "\"\"\"\n",
    "\n",
    "fruit_names = \"\"\"\n",
    "Apple, Pear, this is an kiwi\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Apple': 'ğŸ', 'Pear': 'ğŸ', 'kiwi': 'ğŸ¥'}\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Make your prompt which combines the instructions w/ the fruit names\n",
    "prompt = (instructions + fruit_names)\n",
    "\n",
    "# Call the LLM\n",
    "output = azure_chat_model([HumanMessage(content=prompt)])\n",
    "\n",
    "print (output.content)\n",
    "print (type(output.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Apple': 'ğŸ', 'Pear': 'ğŸ', 'kiwi': 'ğŸ¥'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "output_dict = eval(output.content)\n",
    "\n",
    "print (output_dict)\n",
    "print (type(output_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. è‡ªåŠ¨æ ¼å¼è½¬æ¢\n",
    "\n",
    "è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªå¸¦æœ‰æ ¼å¼è¯´æ˜çš„æç¤º\n",
    "\n",
    "è¿™æ ·å°±ä¸éœ€è¦æ‹…å¿ƒæç¤ºå·¥ç¨‹çš„é—®é¢˜äº†ï¼Œå°†è¿™éƒ¨åˆ†å®Œå…¨äº¤ç»™ Lang Chain æ¥æ‰§è¡Œ\n",
    "\n",
    "å°†LLMçš„è¾“å‡ºè½¬åŒ–ä¸º python å¯¹è±¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(name=\"artist\", description=\"The name of the musical artist\"),\n",
    "    ResponseSchema(name=\"song\", description=\"The name of the song that the artist plays\")\n",
    "]\n",
    "\n",
    "# è§£æå™¨å°†ä¼šæŠŠLLMçš„è¾“å‡ºä½¿ç”¨æˆ‘å®šä¹‰çš„schemaè¿›è¡Œè§£æå¹¶è¿”å›æœŸå¾…çš„ç»“æ„æ•°æ®ç»™æˆ‘\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿™ä¸ª Prompt ä¸ä¹‹å‰æˆ‘ä»¬æ„å»º Chat Model æ—¶ Prompt ä¸åŒ\n",
    "# è¿™ä¸ª Prompt æ˜¯ä¸€ä¸ª ChatPromptTemplateï¼Œå®ƒä¼šè‡ªåŠ¨å°†æˆ‘ä»¬çš„è¾“å‡ºè½¬åŒ–ä¸º python å¯¹è±¡\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"Given a command from the user, extract the artist and song names \\n \\\n",
    "                                                    {format_instructions}\\n{user_prompt}\")  \n",
    "    ],\n",
    "    input_variables=[\"user_prompt\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a command from the user, extract the artist and song names \n",
      "                                                     The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n",
      "I really like So Young by Portugal. The Man\n"
     ]
    }
   ],
   "source": [
    "fruit_query = prompt.format_prompt(user_prompt=\"I really like So Young by Portugal. The Man\")\n",
    "\n",
    "print (fruit_query.messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artist': 'Portugal. The Man', 'song': 'So Young'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "fruit_output = azure_chat_model(fruit_query.to_messages())\n",
    "output = output_parser.parse(fruit_output.content)\n",
    "\n",
    "print (output)\n",
    "print (type(output))\n",
    "# è¿™é‡Œè¦æ³¨æ„çš„æ˜¯ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„ turbo æ¨¡å‹ï¼Œç”Ÿæˆçš„ç»“æœå¹¶ä¸ä¸€å®šæ˜¯æ¯æ¬¡éƒ½ä¸€è‡´çš„\n",
    "# æ›¿æ¢æˆgpt4æ¨¡å‹å¯èƒ½æ˜¯æ›´å¥½çš„é€‰æ‹©"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
